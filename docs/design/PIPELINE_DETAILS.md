# Deep Dive: UrbanLore Pipeline Details

This document provides a detailed technical breakdown of how the UrbanLore pipeline synthesizes datasets and ensures the model learns the correct response format.

## 1. Dataset Synthesis

The dataset is generated by the `QA Generator` agent (`agents/qa_generator.py`) using two distinct strategies:

### A. Fact-Based QA Generation (Knowledge Ingestion)
- **Source**: `corpus/facts.json` (Structured facts extracted from the city lore).
- **Process**: The agent samples facts (Entity, Attribute, Value) and uses an LLM (GPT-4) to phrase them into natural-sounding questions and detailed answers.
- **Goal**: This teaches the model the "ground truth" facts about the fictional city.
- **Format**:
  ```text
  Question: [Natural Language Question]
  Answer: [Detailed Answer based on Fact Context]
  ```

### B. Instruction-Following Generation (Reasoning & Skill)
- **Source**: `corpus/city_corpus.txt` (Raw narrative text).
- **Process**: The agent selects random paragraphs and applies instruction templates (Summarize, Elaborate, Extract). It uses an LLM to generate the "Gold" response.
- **Goal**: This teaches the model how to perform tasks *using* the city lore as context.
- **Format**:
  ```text
  ### Instruction:
  [Task description, e.g., "Summarize the following passage:"]

  ### Input:
  [Source text from the corpus]

  ### Response:
  [Generated gold response]
  ```

## 2. Chat Format Alignment

To ensure the model responds correctly during inference, the pipeline keeps the data format identical between training and evaluation.

### Training Format
The `SFTTrainer` in `finetune/train.py` consumes the "text" field from the JSONL dataset. Because the headers (`Question:`, `### Instruction:`, etc.) are included directly in the training strings, the model learns to associate these specific tokens with the end of the user input and the start of its own response.

### Inference & Evaluation Alignment
During evaluation (`eval/evaluate.py`), the script replicates this exact format:
1.  **Splitting**: It takes a test sample and splits it at the last header (`Answer:` or `### Response:`).
2.  **Prompting**: Only the text *before* (and including) the header is sent to the model.
3.  **Completion**: The model effectively performs a "text completion" task, starting its generation exactly where the training data would have started the answer.

### Why this works
By embedding the "chat structure" into the raw text instead of relying on a complex external ChatML template, the pipeline maintains high compatibility with smaller models (like Phi-2 or Qwen-0.5B) which are highly sensitive to formatting consistency but may not have robust built-in ChatML support.

## 3. Data Integrity & Validation
- **Metadata Tracking**: Every dataset generation run creates a `dataset_metadata.json` file to track the balance between QA pairs and Instruction pairs.
- **Deduplication**: The fact-extraction phase minimizes redundant facts before they ever reach the QA generation stage, ensuring higher variety in the training data.
